{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eqIbSTaCBo8",
        "outputId": "9e018a1d-d7c5-49dd-c49a-38d316d9cdf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Retrieval-Augmented Generation (RAG) using LangChain, LlamaIndex, and OpenAI | by Prasad Mahamulkar | Medium\n",
            "Prasad Mahamulkar\n",
            "Follow\n",
            "--\n",
            "2\n",
            "Listen\n",
            "Share\n",
            "Large Language Models (LLMs) demonstrate significant capabilities but sometimes generate incorrect but believable responses when they lack information, and this is known as “hallucination.” It means they confidently provide information that may sound accurate but could be incorrect due to outdated knowledge.\n",
            "Retrieval-Augmented Generation or RAG framework solves this problem by integrating an information retrieval system into the LLM pipeline. Instead of relying on pre-trained knowledge, RAG allows the model to dynamically fetch information from external knowledge sources when generating responses. This dynamic retrieval mechanism ensures that the information provided by the LLM is not only contextually relevant but also accurate and up-to-date.\n",
            "It is a more efficient way to provide additional or domain-specific information using an external database, rather than repeatedly retraining or fine-tuning the model on updated data.\n",
            "In this article, we will understand how RAG works and create our own basic and Advanced RAG systems using LangChain and LlamaIndex.\n",
            "Now let's start with understanding how RAG works.\n",
            "The basic RAG workflow is illustrated below:\n",
            "The indexing process is a crucial first step in data preparation for language models. Original data is cleaned, converted into standardized plain text, and segmented into smaller chunks for efficient processing. These chunks are transformed into vector representations through an embedding model, facilitating similarity comparisons during retrieval. The final index stores these text chunks and their vector embeddings, enabling efficient and scalable search capabilities.\n",
            "When a user asks a question, the system uses the encoding model from the indexing phase to transcode it. Next, it calculates similarity scores between the query vector and vectorized chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks showing the highest similarity, using them as an expanded contextual basis to address the user’s request.\n",
            "The user’s question and chosen documents are combined into a clear prompt for a large language model. Then model crafts a response, adapting its approach based on task-specific criteria.\n",
            "Now let's see how to implement the basic RAG technique using LangChain and LlamaIndex.\n",
            "Google Colab Notebook\n",
            "Step 1: Start by installing and loading all the necessary libraries.\n",
            "Step 2: Next, load a PDF document using PyPDFLoader to extract text from PDF files.\n",
            "Step 3: Once the PDF is loaded, use the TextSplitter to split the document into chunks.\n",
            "Step 4: Now, load an embedding model to convert text into numerical embeddings. Here, we are using the “BAAI/bge-small-en-v1.5” embedding model, but you can choose any model from the Hugging Face embedding models.\n",
            "Step 5: Create a Vector Store using FAISS to store embeddings and text chunks. If you want you can save these embeddings for later use.\n",
            "Step 6: Now, create a retriever using the vector store. This step establishes the foundation for information retrieval based on vector similarities.\n",
            "Step 7: Load the Language Model (LLM) that you want to use for retrieval and create a document chain.\n",
            "Step 8: Finally, create a retrieval chain by combining the retriever and document chain. Invoke the chain with a user query to get a relevant response.\n",
            "Now let's move on to basic RAG with LlamaIndex\n",
            "Step 1: Start by installing and loading all the necessary libraries from llamaIndex.\n",
            "Step 2: Load a PDF document and combine each page of the document into one document object.\n",
            "Step 3: Now, split the document into text chunks. These chunks are known as “Nodes” in LlamaIndex. Also, reset default node IDs for better understanding.\n",
            "Step 4: Load an embedding model and language model (LLM). We are using the same models that we used for LangChain.\n",
            "Step 5: Create a service by bundling LLM and embedding model for the indexing and querying stage.\n",
            "Step 6: Create and store embeddings of nodes (chunks) and store them in the vector store index.\n",
            "Step 7: Create a retriever using the vector store index to retrieve relevant information for user queries.\n",
            "Step 8: Finally, set up a query engine by combining the retriever and service context, and add a user query to get a relevant response.\n",
            "Now let’s take a look at how to improve LLM response using Advanced RAG.\n",
            "The problem with the basic RAG technique is that, as document size increases, embeddings become larger and more complex, which can reduce the specificity and contextual meaning of a document. To solve this problem, we use the advanced RAG technique called Parent Document Retriever.\n",
            "Parent Document Retriever creates small and more accurate embeddings while retaining the contextual meaning of large documents. Parent document retriever helps LLM by using details from child documents for accurate retrieval and gaining additional context from parent documents during generation. This makes language models better at providing detailed and comprehensive answers.\n",
            "Let’s see how to implement the Parent Document Retriever technique using LangChain and LlamaIndex.\n",
            "Parent Document Retriever LangChain Documentation.\n",
            "Update the following steps in the basic RAG process.\n",
            "Step 3: Use the TextSplitter to split the document into parent and child chunks.\n",
            "Step 5: Create a Vector Store using Chromadb to store new embeddings and text chunks.\n",
            "Step 6: Create a Parent doc retriever then, add a document to the retriever.\n",
            "Step 8: Finally, create a retrieval chain, similar to the previous chain, and invoke it with a user query to get a response.\n",
            "As you can see in the output, we get a more detailed response compared to the basic RAG method.\n",
            "Parent Document Reteriver LlamaIndex Documentation.\n",
            "Update the following steps in the basic RAG.\n",
            "Step 3: In this section, we set child chunk sizes (128, 256, 512) in `sub_chunk_sizes` and create parsers (`sub_node_parsers`) for child chunks. These parsers, with specific sizes, will be used to parse child chunks from the original document.\n",
            "We then iterate over each original document chunk (`base_node`). For each base node, we use parsers to generate child nodes, linked to their parent node. Parent nodes and child nodes are added to the `all_nodes` list.\n",
            "Finally, a dictionary (`all_nodes_dict`) is created for easy access to nodes based on their IDs. This helps efficiently retrieve information.\n",
            "Step 6: Create embeddings of all nodes (which contain both parent and child nodes) and store them in the vector store index.\n",
            "Step 7: In the Retriever process, we use a RecursiveRetriever that navigates node connections based on “references.” This retriever explores links from nodes to other retrievers. If the retrieved nodes include IndexNodes, it further explores linked retrievers and conducts queries. This recursive approach helps gather information effectively.\n",
            "Step 8: Finally, set up a query engine by combining the retriever and service context, and add a user query.\n",
            "Here also, we get a more detailed response compared to basic RAG.\n",
            "In this article, we explored the fundamentals of RAG and successfully developed both basic and Advanced RAG systems using LangChain and LlamaIndex. I hope you found this article useful. I recommend exploring other Advanced RAG techniques and working with different data types (like CSV) to gain more experience.\n",
            "Retrieval-Augmented Generation for LLMs: A Survey\n",
            "Parent Document Retriever langChain Documentation\n",
            "Parent Document Reteriver LlamaIndex Documentation\n",
            "Subscribe for free to get notified when I publish a new article.\n",
            "medium.com\n",
            "You can also find me on LinkedIn and Twitter!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = requests.get('https://medium.com/@prasadmahamulkar/introduction-to-retrieval-augmented-generation-rag-using-langchain-and-lamaindex-bd0047628e2a')\n",
        "\n",
        "\n",
        "if response.status_code == 200:\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the title of the article\n",
        "    title = soup.title.string\n",
        "    print(f\"Title: {title}\")\n",
        "\n",
        "    # Extract the article content\n",
        "    article_body = soup.find('article')\n",
        "    if article_body:\n",
        "        paragraphs = article_body.find_all('p')\n",
        "        for paragraph in paragraphs:\n",
        "            print(paragraph.get_text())\n",
        "else:\n",
        "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
      ]
    }
  ]
}